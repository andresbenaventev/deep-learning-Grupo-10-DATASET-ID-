{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mcyPkl8etAu"
   },
   "source": [
    "# Gu\u00eda estructurada de la Tarea 1\n",
    "\n",
    "Este notebook resume los hitos principales del enunciado para mantener un plan de trabajo ordenado. Avanza secci\u00f3n por secci\u00f3n y completa cada hito conforme desarrolles tu soluci\u00f3n del proyecto.\n"
   ],
   "id": "5mcyPkl8etAu"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nl_xPaNWetAw"
   },
   "outputs": [],
   "source": [
    "# Configuraci\u00f3n base para reproducibilidad\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "GLOBAL_SEED = 42\n",
    "DATASET_ID = \"010\"\n",
    "\n",
    "random.seed(GLOBAL_SEED)\n",
    "np.random.seed(GLOBAL_SEED)\n"
   ],
   "id": "Nl_xPaNWetAw"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3nDSy1YqetAz"
   },
   "source": [
    "## 1. E\u2013T\u2013P y framing\n",
    "\n",
    "- Describe la Experiencia (E), la Tarea (T) y el Performance/criterio (P).\n",
    "- Define el framing principal como clasificaci\u00f3n binaria de `purchase` con salida probabil\u00edstica.\n",
    "- Justifica si usar\u00e1s tasas agregadas y la decisi\u00f3n de negocio basada en umbrales.\n",
    "\n"
   ],
   "id": "3nDSy1YqetAz"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KIjYp-BqetA1"
   },
   "source": [
    "## 2. M\u00e9tricas y p\u00e9rdida\n",
    "\n",
    "- Usa log-loss (entrop\u00eda cruzada) como p\u00e9rdida principal.\n",
    "- Reporta AUC y Brier score.\n",
    "- Explica por qu\u00e9 MSE no es adecuado como objetivo principal y relaci\u00f3nalo con m\u00e1xima verosimilitud.\n",
    "\n"
   ],
   "id": "KIjYp-BqetA1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEuuPE1ZetA1"
   },
   "source": [
    "## 3. Dise\u00f1o de validaci\u00f3n y control de capacidad\n",
    "\n",
    "- Define particiones train/valid/test (70/15/15) o k-fold para escoger hiperpar\u00e1metros.\n",
    "- Reentrena con los mejores hiperpar\u00e1metros antes del test.\n",
    "- Controla capacidad con regularizaci\u00f3n L2 (par\u00e1metro C) y grafica curvas train/valid vs. complejidad o learning curves.\n",
    "- Explica el trade-off sesgo\u2013varianza.\n",
    "\n"
   ],
   "id": "vEuuPE1ZetA1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-N5jkwjetA1"
   },
   "source": [
    "## 4. Preprocesamiento\n",
    "\n",
    "- Explora el dataset para identificar duplicados, variables irrelevantes o leakage.\n",
    "- Define codificaci\u00f3n de variables categ\u00f3ricas, escalamiento y manejo de outliers o nulos seg\u00fan corresponda.\n",
    "\n"
   ],
   "id": "Y-N5jkwjetA1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amvvIh7DetA1"
   },
   "source": [
    "## 5. Modelado predictivo\n",
    "\n",
    "- Entrena al menos dos modelos de clasificaci\u00f3n (ej. regresi\u00f3n log\u00edstica, \u00e1rbol, random forest, XGBoost).\n",
    "- Compara su desempe\u00f1o inicial.\n",
    "\n"
   ],
   "id": "amvvIh7DetA1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TW7GqU-getA2"
   },
   "source": [
    "## 6. Evaluaci\u00f3n\n",
    "\n",
    "- Reporta m\u00e9tricas de clasificaci\u00f3n: accuracy, precision, recall, F1-score y AUC-ROC.\n",
    "\n"
   ],
   "id": "TW7GqU-getA2"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlrEMA49etA2"
   },
   "source": [
    "## 7. Discusi\u00f3n de resultados\n",
    "\n",
    "- Destaca variables relevantes para los modelos.\n",
    "- Justifica columnas excluidas (especialmente leaks).\n",
    "- Prop\u00f3n insights accionables para la empresa.\n",
    "\n"
   ],
   "id": "HlrEMA49etA2"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHPL15I0etA2"
   },
   "source": [
    "## 8. Pol\u00edtica operativa y sensibilidad\n",
    "\n",
    "- Formula una regla clara: contactar/ofrecer si la probabilidad estimada supera el umbral t.\n",
    "- Analiza sensibilidad (ej. utilidad esperada por umbral) y discute implicancias.\n",
    "\n"
   ],
   "id": "oHPL15I0etA2"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gMP1LHp4etA2"
   },
   "source": [
    "## 9. Riesgos y mitigaci\u00f3n\n",
    "\n",
    "- Identifica al menos tres riesgos: leakage, sesgo de muestreo, shift temporal/segmento.\n",
    "- Prop\u00f3n mitigaciones (auditor\u00eda de variables, validaci\u00f3n por segmento o fuera de tiempo, calibration, A/B).\n",
    "\n"
   ],
   "id": "gMP1LHp4etA2"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKuY-5JnetA3"
   },
   "source": [
    "## 10. Resultados y conclusiones\n",
    "\n",
    "- Resume hallazgos clave: saturaci\u00f3n, desempe\u00f1o en test, umbral recomendado.\n",
    "- Indica c\u00f3mo debe operar la empresa con el modelo final.\n",
    "\n"
   ],
   "id": "VKuY-5JnetA3"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aA49wVLNetA3"
   },
   "source": [
    "## Extras opcionales\n",
    "\n",
    "- Mostrar efectos de usar `leak_after_offer` para evidenciar data leakage.\n",
    "- Calcular m\u00e9tricas de negocio (ej. expected profit) usando `unit_margin_if_buy`.\n",
    "- Comparar `discount` num\u00e9rico vs. `discount_bucket`.\n",
    "\n"
   ],
   "id": "aA49wVLNetA3"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pC8fBNGxetA3"
   },
   "source": [
    "## Formato y reproducibilidad\n",
    "\n",
    "- Declara semilla global y DATASET_ID en la primera celda.\n",
    "- Fija semillas para NumPy/sklearn y rep\u00f3rtalas.\n",
    "- Reporta versiones de librer\u00edas y hash SHA-256 del CSV.\n",
    "- Trabaja s\u00f3lo con el dataset entregado, sin regenerar datos.\n",
    "- Entrega resultados espec\u00edficos de tu dataset.\n",
    "\n"
   ],
   "id": "pC8fBNGxetA3"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Punto 1: E\u2013T\u2013P y framing del proyecto\n",
    "import textwrap\n",
    "import pandas as pd\n",
    "\n",
    "dataset_path = Path(f\"T1_{DATASET_ID}_individual.csv\")\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "FEATURE_COLUMNS = [\n",
    "    'segment',\n",
    "    'discount',\n",
    "    'age',\n",
    "    'tenure_months',\n",
    "    'income_index',\n",
    "    'web_visits_30d',\n",
    "    'unit_margin_if_buy',\n",
    "    'discount_bucket',\n",
    "]\n",
    "TARGET_COLUMN = 'purchase'\n",
    "CATEGORICAL_FEATURES = ['segment', 'discount_bucket']\n",
    "NUMERICAL_FEATURES = [\n",
    "    'discount',\n",
    "    'age',\n",
    "    'tenure_months',\n",
    "    'income_index',\n",
    "    'web_visits_30d',\n",
    "    'unit_margin_if_buy',\n",
    "]\n",
    "\n",
    "etp_framing = {\n",
    "    'experiencia': (\n",
    "        'Campa\u00f1a outbound para clientes del segmento de retail financiero en la que cada fila del dataset '\n",
    "        'representa un contacto individual previo al env\u00edo de una oferta de descuento.'\n",
    "    ),\n",
    "    'tarea': (\n",
    "        'Predecir si el cliente concretar\u00e1 la compra del producto ofrecido (variable purchase) a partir de '\n",
    "        'la informaci\u00f3n disponible antes de realizar la oferta.'\n",
    "    ),\n",
    "    'performance': (\n",
    "        'Evaluar la calidad de las predicciones probabil\u00edsticas minimizando log-loss y maximizando m\u00e9tricas '\n",
    "        'discriminativas como AUC, para tomar decisiones comerciales basadas en umbrales.'\n",
    "    ),\n",
    "    'framing': (\n",
    "        'Se modelar\u00e1 como un problema de clasificaci\u00f3n binaria con salida probabil\u00edstica sobre purchase. '\n",
    "        'Esto permite fijar umbrales de contacto para priorizar a los clientes con mayor propensi\u00f3n.'\n",
    "    ),\n",
    "    'politica_umbral': (\n",
    "        'La operaci\u00f3n aplicar\u00e1 un umbral sobre la probabilidad estimada: se contacta al cliente solo si la '\n",
    "        'probabilidad supera t, lo que permite gestionar la capacidad y maximizar retorno esperado.'\n",
    "    ),\n",
    "}\n",
    "\n",
    "for clave, descripcion in etp_framing.items():\n",
    "    print(clave.upper())\n",
    "    print(textwrap.fill(descripcion, width=100))\n",
    "    print()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Punto 2: M\u00e9tricas, p\u00e9rdida y justificaci\u00f3n\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import brier_score_loss, log_loss, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('categorical', OneHotEncoder(handle_unknown='ignore'), CATEGORICAL_FEATURES),\n",
    "        ('numeric', Pipeline([('scaler', StandardScaler())]), NUMERICAL_FEATURES),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', LogisticRegression(random_state=GLOBAL_SEED, max_iter=1000)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "X = df[FEATURE_COLUMNS]\n",
    "y = df[TARGET_COLUMN]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=GLOBAL_SEED\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "metricas = {\n",
    "    'log_loss': log_loss(y_test, y_proba),\n",
    "    'auc': roc_auc_score(y_test, y_proba),\n",
    "    'brier_score': brier_score_loss(y_test, y_proba),\n",
    "}\n",
    "\n",
    "for nombre, valor in metricas.items():\n",
    "    print(f\"{nombre}: {valor:.4f}\")\n",
    "\n",
    "mse_explicacion = (\n",
    "    'La p\u00e9rdida log-loss coincide con la maximizaci\u00f3n de la verosimilitud de un modelo Bernoulli y penaliza con mayor '\n",
    "    'fuerza las probabilidades mal calibradas. Usar MSE como objetivo para clasificaci\u00f3n binaria rompe esta relaci\u00f3n '\n",
    "    'probabil\u00edstica, produce gradientes pobres cerca de los extremos y no prioriza la calibraci\u00f3n, por lo que no es '\n",
    "    'adecuado como criterio principal.'\n",
    ")\n",
    "\n",
    "print('\nJustificaci\u00f3n sobre el rechazo de MSE:')\n",
    "print(textwrap.fill(mse_explicacion, width=100))\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}